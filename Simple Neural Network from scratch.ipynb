{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stop criterion reached after 271 epochs\n",
            "Train Accuracy =  0.785875\n",
            "Validation Accuracy =  0.7808333333333334\n",
            "Test Accuracy =  0.7939\n",
            "Costs:  [{'Step 100': 3.061221368422179}, {'Step 200': 2.5833868158406403}, {'Step 300': 2.574453038951019}, {'Step 400': 2.5372178368863034}, {'Step 500': 2.4319804882206175}, {'Step 600': 2.2315156512018404}, {'Step 700': 2.25172659536874}, {'Step 800': 2.305951450636306}, {'Step 900': 2.1157310170394745}, {'Step 1000': 2.082152374094138}, {'Step 1100': 2.0094809763924317}, {'Step 1200': 1.9780949053525554}, {'Step 1300': 1.834433266333368}, {'Step 1400': 1.9147221448393283}, {'Step 1500': 2.0041870257895185}, {'Step 1600': 1.9498583118342048}, {'Step 1700': 1.89568394274596}, {'Step 1800': 1.7932269031413106}, {'Step 1900': 1.6998971974108454}, {'Step 2000': 1.5979601861592418}, {'Step 2100': 1.6707318966410636}, {'Step 2200': 1.8077101244714513}, {'Step 2300': 1.7689401412778156}, {'Step 2400': 1.747732123860466}, {'Step 2500': 1.6420903961380062}, {'Step 2600': 1.524911169431079}, {'Step 2700': 1.4510053676759322}, {'Step 2800': 1.4656496908214054}, {'Step 2900': 1.6278178468478908}, {'Step 3000': 1.6326854447483934}, {'Step 3100': 1.6114879297698481}, {'Step 3200': 1.4999000719007027}, {'Step 3300': 1.397739208364972}, {'Step 3400': 1.2808976969935921}, {'Step 3500': 1.2967974701816687}, {'Step 3600': 1.4985833588641668}, {'Step 3700': 1.4702495937211906}, {'Step 3800': 1.4745569730583705}, {'Step 3900': 1.3716275040682142}, {'Step 4000': 1.2799074934858077}, {'Step 4100': 1.161303507839034}, {'Step 4200': 1.1599136192988881}, {'Step 4300': 1.3824483769289602}, {'Step 4400': 1.3461688070070708}, {'Step 4500': 1.355470110375033}, {'Step 4600': 1.2410085097488217}, {'Step 4700': 1.1703003593011216}, {'Step 4800': 1.0449321033427312}, {'Step 4900': 1.0497801265178066}, {'Step 5000': 1.23474278447917}, {'Step 5100': 1.211637700459113}, {'Step 5200': 1.2604861924989372}, {'Step 5300': 1.1244984140576175}, {'Step 5400': 1.0719561991396869}, {'Step 5500': 0.9367596524370013}, {'Step 5600': 0.9474210125953834}, {'Step 5700': 1.1419148144317115}, {'Step 5800': 1.0915611184417913}, {'Step 5900': 1.1702826401246307}, {'Step 6000': 1.0118073372544163}, {'Step 6100': 0.9748357976370589}, {'Step 6200': 0.8424898079824312}, {'Step 6300': 0.869757827820127}, {'Step 6400': 1.0529083571648437}, {'Step 6500': 0.9882586293119644}, {'Step 6600': 1.089790172422944}, {'Step 6700': 0.9217923309429463}, {'Step 6800': 0.8890666526842098}, {'Step 6900': 0.7616560390049314}, {'Step 7000': 0.8048033271440184}, {'Step 7100': 0.9731234641210145}, {'Step 7200': 0.8976386618836201}, {'Step 7300': 1.0081710459820208}, {'Step 7400': 0.8380584241053988}, {'Step 7500': 0.8280596877653298}]\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def initialize_parameters(layer_dims):\n",
        "    # Initializing the variables\n",
        "    np.random.seed(1)\n",
        "    parameters = {}\n",
        "    # Create each layer based on the dimentions received\n",
        "    for layer in range(1, len(layer_dims)):\n",
        "        n = layer_dims[layer - 1]\n",
        "        parameters[\"W\" + str(layer)] = np.random.randn(\n",
        "            layer_dims[layer], layer_dims[layer - 1]\n",
        "        ) * np.sqrt(2 / n)\n",
        "        parameters[\"b\" + str(layer)] = np.zeros((layer_dims[layer], 1))\n",
        "    return parameters\n",
        "\n",
        "\n",
        "def linear_forward(A, W, b):\n",
        "    # Calculate Z\n",
        "    Z = np.dot(W, A) + b\n",
        "    linear_cache = {\"A\": A, \"W\": W, \"b\": b}\n",
        "    return Z, linear_cache\n",
        "\n",
        "\n",
        "def softmax(Z):\n",
        "    # Apply the softmax activation function\n",
        "    exp = np.exp(Z - np.max(Z))\n",
        "    A = exp / np.sum(exp, axis=0)\n",
        "    return A, Z\n",
        "\n",
        "\n",
        "def relu(Z):\n",
        "    # Apply the relu activation function\n",
        "    A = np.maximum(0, Z)\n",
        "    activation_cache = Z\n",
        "    return A, activation_cache\n",
        "\n",
        "\n",
        "def linear_activation_forward(A_prev, W, B, activation):\n",
        "    # Initializing the variables\n",
        "    activation_function = {\"softmax\": softmax, \"relu\": relu}\n",
        "    Z, linear_cache = linear_forward(A_prev, W, B)\n",
        "    # Apply the activation function\n",
        "    A, activation_cache = activation_function[activation](Z)\n",
        "    layer_cache = {\"linear_cache\": linear_cache, \"activation_cache\": activation_cache}\n",
        "    return A, layer_cache\n",
        "\n",
        "\n",
        "def apply_batchnorm(A):\n",
        "    epsilon = 0.00000001\n",
        "    norm = (A - A.mean()) / np.sqrt(A.var() + epsilon)\n",
        "    return norm\n",
        "\n",
        "\n",
        "def L_model_forward(X, parameters, use_batchnorm=True):\n",
        "    # Initializng the variables\n",
        "    caches = []\n",
        "    A = X\n",
        "    layers = len(parameters) // 2\n",
        "    # Activate all layers except of the last with relu\n",
        "    for layer in range(1, layers):\n",
        "        A, layer_cache = linear_activation_forward(\n",
        "            A, parameters[\"W\" + str(layer)], parameters[\"b\" + str(layer)], \"relu\"\n",
        "        )\n",
        "        caches.append(layer_cache)\n",
        "        A = A if not use_batchnorm else apply_batchnorm(A)\n",
        "    # Activate the last layer with softmax\n",
        "    AL, layer_cache = linear_activation_forward(\n",
        "        A,\n",
        "        parameters[\"W\" + str(layers)],\n",
        "        parameters[\"b\" + str(layers)],\n",
        "        \"softmax\",\n",
        "    )\n",
        "    caches.append(layer_cache)\n",
        "    return AL, caches\n",
        "\n",
        "\n",
        "\n",
        "def compute_cost(AL, Y):\n",
        "    batch_size = Y.shape[1]\n",
        "    # Calculating cost\n",
        "    cost = -1.0 / batch_size * np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL))\n",
        "    return cost\n",
        "\n",
        "\n",
        "def Linear_backward(dZ, cache):\n",
        "    # Initializng the variables\n",
        "    A_prev = cache[\"A\"]\n",
        "    W = cache[\"W\"]\n",
        "    batch_size = A_prev.shape[1]\n",
        "    # Calculating previous dA, dW, db\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "    dW = 1.0 / batch_size * np.dot(dZ, A_prev.T)\n",
        "    db = 1.0 / batch_size * np.sum(dZ, axis=1, keepdims=True)\n",
        "    return dA_prev, dW, db\n",
        "\n",
        "\n",
        "def relu_backward(dA, activation_cache):\n",
        "    # Calculates the relu derivative\n",
        "    dZ = np.array(dA)\n",
        "    dZ[activation_cache <= 0] = 0\n",
        "    return dZ\n",
        "\n",
        "\n",
        "def softmax_backward(dA, activation_cache):\n",
        "    # Calculates the softmax derivative\n",
        "    sm, cache = softmax(activation_cache)\n",
        "    dZ = dA * (sm * (1 - sm))\n",
        "    return dZ\n",
        "\n",
        "\n",
        "def linear_activation_backward(dA, cache, activation):\n",
        "    # Initializing the variables\n",
        "    linear_cache = cache[\"linear_cache\"]\n",
        "    activation_cache = cache[\"activation_cache\"]\n",
        "    activation_backward_functions = {\"relu\": relu_backward, \"softmax\": softmax_backward}\n",
        "    # Apply the activation function\n",
        "    dZ = activation_backward_functions[activation](dA, activation_cache)\n",
        "    return Linear_backward(dZ, linear_cache)\n",
        "\n",
        "\n",
        "def L_model_backward(AL, Y, caches):\n",
        "    # Initializng the variables\n",
        "    grads = {}\n",
        "    layers = len(caches)\n",
        "    # To make sure Y has the same shape as AL\n",
        "    Y = Y.reshape(AL.shape)\n",
        "    # Calculates the post activation gradient\n",
        "    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "    # For the last layer, apply softmax\n",
        "    cache = caches[layers - 1]\n",
        "    (\n",
        "        grads[\"dA\" + str(layers - 1)],\n",
        "        grads[\"dW\" + str(layers)],\n",
        "        grads[\"db\" + str(layers)],\n",
        "    ) = linear_activation_backward(dAL, cache, \"softmax\")\n",
        "    # For all other layers, apply relu\n",
        "    for layer in reversed(range(1, layers)):\n",
        "        cache = caches[layer - 1]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(\n",
        "            grads[\"dA\" + str(layer)], cache, \"relu\"\n",
        "        )\n",
        "        grads[\"dA\" + str(layer - 1)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(layer)] = dW_temp\n",
        "        grads[\"db\" + str(layer)] = db_temp\n",
        "    return grads\n",
        "\n",
        "\n",
        "def Update_parameters(parameters, grads, learning_rate):\n",
        "    # Updates the weights and biases of each layer, based on the learning rate\n",
        "    for layer in range(1, (len(parameters) // 2) + 1):\n",
        "        parameters[\"W\" + str(layer)] = (\n",
        "            parameters[\"W\" + str(layer)] - learning_rate * grads[\"dW\" + str(layer)]\n",
        "        )\n",
        "        parameters[\"b\" + str(layer)] = (\n",
        "            parameters[\"b\" + str(layer)] - learning_rate * grads[\"db\" + str(layer)]\n",
        "        )\n",
        "    return parameters\n",
        "\n",
        "\n",
        "def L_layer_model(X, Y, layers_dims, learning_rate, num_iterations, batch_size):\n",
        "    # Initializing variables\n",
        "    costs = []\n",
        "    parameters = initialize_parameters(layers_dims)\n",
        "    x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.2)\n",
        "    stop_count = 0\n",
        "    step = 0\n",
        "    val_acc = 0\n",
        "    y_temp = to_categorical(y_train)\n",
        "    # Run until iteration limit reached\n",
        "    for i in range(1, num_iterations + 1):\n",
        "        # Run on all batches\n",
        "        for j in range(math.ceil(X.shape[1] / batch_size)):\n",
        "            step +=1\n",
        "            start = j * batch_size\n",
        "            end = (j + 1) * batch_size\n",
        "            x_batch = x_train.T[:, start:end]\n",
        "            y_batch = y_temp.T[:, start:end]\n",
        "            # Forward propagation\n",
        "            AL, caches = L_model_forward(x_batch, parameters)\n",
        "            # Cost computation\n",
        "            cost = compute_cost(AL, y_batch)\n",
        "            # Backward propagation\n",
        "            grads = L_model_backward(AL, y_batch, caches)\n",
        "            # Parameters updated\n",
        "            parameters = Update_parameters(parameters, grads, learning_rate)\n",
        "            # Check validation set accuracy\n",
        "            temp = Predict(x_val, y_val, parameters)\n",
        "            # Save cost each 100 training steps\n",
        "            if step % 100 == 0:\n",
        "                costs.append({\"Step \" +str(step):cost})\n",
        "            # Count steps with small or no improvement\n",
        "            if temp <= val_acc + 0.0001:\n",
        "                stop_count += 1\n",
        "            else:\n",
        "                val_acc = temp\n",
        "                stop_count = 0\n",
        "            # If stop criterion reached, break the loop\n",
        "            if stop_count >= 100:\n",
        "                break\n",
        "        # If stop criterion reached, break the loop\n",
        "        if stop_count >= 100:\n",
        "            print(\"Stop criterion reached after \"+ str(i)+\" epochs\")\n",
        "            break\n",
        "    # Print the model accuracy on the train and validation sets\n",
        "    print(\"Train Accuracy = \", str(Predict(x_train, y_train, parameters)))\n",
        "    print(\"Validation Accuracy = \", str(Predict(x_val, y_val, parameters)))\n",
        "    return parameters, costs\n",
        "\n",
        "\n",
        "def Predict(X, y, parameters):\n",
        "    # Gets the probabilities for each sample\n",
        "    probs, caches = L_model_forward(X.T, parameters)\n",
        "    probs = probs.T\n",
        "    # Gets the predicted categorization of each sample\n",
        "    pr = np.argmax(probs, axis=1)\n",
        "    # Calculates the accuracy based on correct predictions\n",
        "    acc = np.mean(pr == y)\n",
        "    return acc\n",
        "\n",
        "# Loads the data and flattens the input\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_train = x_train / 255\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "x_test = x_test / 255\n",
        "# Initializing hyperparameters\n",
        "dims = [x_train.shape[1], 20, 7, 5, 10]\n",
        "learn_rate = 0.009\n",
        "iters = 750\n",
        "batch_size = 28\n",
        "params, costs = L_layer_model(x_train, y_train, dims, learn_rate, iters, batch_size)\n",
        "# Print the model accuracy on the test set\n",
        "print(\"Test Accuracy = \", str(Predict(x_test, y_test, params)))\n",
        "# Print the costs of the 100th training step iterations\n",
        "print(\"Costs: \",costs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
